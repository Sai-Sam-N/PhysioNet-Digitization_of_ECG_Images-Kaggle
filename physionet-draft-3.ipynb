{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97984,"databundleVersionId":14096757,"sourceType":"competition"},{"sourceId":677607,"sourceType":"modelInstanceVersion","modelInstanceId":513841,"modelId":528480}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:23.448588Z","iopub.execute_input":"2026-01-14T19:32:23.448984Z","iopub.status.idle":"2026-01-14T19:32:29.237909Z","shell.execute_reply.started":"2026-01-14T19:32:23.448951Z","shell.execute_reply":"2026-01-14T19:32:29.236602Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Model added : physio-seg-public","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.240704Z","iopub.execute_input":"2026-01-14T19:32:29.241184Z","iopub.status.idle":"2026-01-14T19:32:29.245594Z","shell.execute_reply.started":"2026-01-14T19:32:29.241153Z","shell.execute_reply":"2026-01-14T19:32:29.244521Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Kaggle paths\nDATA_DIR = r\"/kaggle/input/physionet-ecg-image-digitization/\"\nMODEL_DIR = r\"/kaggle/input/physio-seg-public/pytorch/net3_009_4200/1\"\nTEST_IMG_DIR = os.path.join(DATA_DIR, r\"test\")\nTEST_CSV = os.path.join(DATA_DIR, r\"test.csv\")\nSAMPLE_SUB = os.path.join(DATA_DIR, r\"sample_submission.parquet\")\nOUT_SUB = r\"/kaggle/working/submission.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.246895Z","iopub.execute_input":"2026-01-14T19:32:29.247313Z","iopub.status.idle":"2026-01-14T19:32:29.265303Z","shell.execute_reply.started":"2026-01-14T19:32:29.247278Z","shell.execute_reply":"2026-01-14T19:32:29.264220Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.266598Z","iopub.execute_input":"2026-01-14T19:32:29.266988Z","iopub.status.idle":"2026-01-14T19:32:29.286412Z","shell.execute_reply.started":"2026-01-14T19:32:29.266956Z","shell.execute_reply":"2026-01-14T19:32:29.285378Z"}},"outputs":[{"name":"stdout","text":"Device: cpu\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Model - Minimal UNet Blocks","metadata":{}},{"cell_type":"code","source":"class ConvBNReLU(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.287682Z","iopub.execute_input":"2026-01-14T19:32:29.288062Z","iopub.status.idle":"2026-01-14T19:32:29.304097Z","shell.execute_reply.started":"2026-01-14T19:32:29.288022Z","shell.execute_reply":"2026-01-14T19:32:29.303033Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def center_crop(tensor, target_h, target_w):\n    _, _, h, w = tensor.shape\n    dh = (h - target_h) // 2\n    dw = (w - target_w) // 2\n    return tensor[:, :, dh:dh + target_h, dw:dw + target_w]\n\ndef match_tensor(x, ref):\n    \"\"\"\n    Crops x spatially so that x.shape[-2:] == ref.shape[-2:]\n    \"\"\"\n    _, _, h, w = x.shape\n    _, _, rh, rw = ref.shape\n\n    dh = h - rh\n    dw = w - rw\n\n    if dh > 0:\n        x = x[:, :, dh // 2 : h - (dh - dh // 2), :]\n    if dw > 0:\n        x = x[:, :, :, dw // 2 : w - (dw - dw // 2)]\n\n    return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.305449Z","iopub.execute_input":"2026-01-14T19:32:29.306045Z","iopub.status.idle":"2026-01-14T19:32:29.325831Z","shell.execute_reply.started":"2026-01-14T19:32:29.306003Z","shell.execute_reply":"2026-01-14T19:32:29.324616Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, in_ch, skip_ch, out_ch):\n        super().__init__()\n        self.conv1 = ConvBNReLU(in_ch + skip_ch, out_ch)\n        self.conv2 = ConvBNReLU(out_ch, out_ch)\n    \n    \n    def forward(self, x, skip):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n\n        if x.shape[-2:] != skip.shape[-2:]:\n            skip = match_tensor(skip, x)\n\n        x = torch.cat([x, skip], dim=1)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.328714Z","iopub.execute_input":"2026-01-14T19:32:29.329154Z","iopub.status.idle":"2026-01-14T19:32:29.344006Z","shell.execute_reply.started":"2026-01-14T19:32:29.329126Z","shell.execute_reply":"2026-01-14T19:32:29.342904Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# ResNet34 Encoder (Torchvision-Free)","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        from torchvision.models import resnet34\n        m = resnet34(weights=None)\n\n        self.layer0 = nn.Sequential(m.conv1, m.bn1, m.relu)\n        self.layer1 = nn.Sequential(m.maxpool, m.layer1)\n        self.layer2 = m.layer2\n        self.layer3 = m.layer3\n        self.layer4 = m.layer4\n\n    def forward(self, x):\n        x0 = self.layer0(x)\n        x1 = self.layer1(x0)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n        return x0, x1, x2, x3, x4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.345372Z","iopub.execute_input":"2026-01-14T19:32:29.345781Z","iopub.status.idle":"2026-01-14T19:32:29.362675Z","shell.execute_reply.started":"2026-01-14T19:32:29.345741Z","shell.execute_reply":"2026-01-14T19:32:29.361575Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Full UNet Model (Matches physio-seg)","metadata":{}},{"cell_type":"code","source":"class ECGUNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.encoder = Encoder()\n\n        self.decoder4 = DecoderBlock(512, 256, 256)\n        self.decoder3 = DecoderBlock(256, 128, 128)\n        self.decoder2 = DecoderBlock(128, 64, 64)\n        self.decoder1 = DecoderBlock(64, 64, 16)\n        \n        self.head     = nn.Conv2d(16, 4, kernel_size=1)\n\n\n    def forward(self, x):\n        x0, x1, x2, x3, x4 = self.encoder(x)\n        d4 = self.decoder4(x4, x3)\n        d3 = self.decoder3(d4, x2)\n        d2 = self.decoder2(d3, x1)\n        d1 = self.decoder1(d2, x0)\n        return self.head(d1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.364445Z","iopub.execute_input":"2026-01-14T19:32:29.364858Z","iopub.status.idle":"2026-01-14T19:32:29.381050Z","shell.execute_reply.started":"2026-01-14T19:32:29.364819Z","shell.execute_reply":"2026-01-14T19:32:29.379902Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Load physio-seg Weights","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ECGUNet().to(device)\n\nckpt = torch.load(\n    \"/kaggle/input/physio-seg-public/pytorch/net3_009_4200/1/iter_0004200.pt\",\n    map_location=device\n)\n\nstate = ckpt[\"state_dict\"] if \"state_dict\" in ckpt else ckpt\n\nclean = {}\nfor k, v in state.items():\n    if \"num_batches_tracked\" in k:\n        continue\n    clean[k.replace(\"decoder.block.\", \"decoder.\")\\\n            .replace(\"pixel.\", \"head.\")] = v\n\nmodel.load_state_dict(clean, strict=False)\nmodel.eval()\n\nprint(\"Model loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:29.382318Z","iopub.execute_input":"2026-01-14T19:32:29.382689Z","iopub.status.idle":"2026-01-14T19:32:35.309226Z","shell.execute_reply.started":"2026-01-14T19:32:29.382652Z","shell.execute_reply":"2026-01-14T19:32:35.308251Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Image Preprocessing","metadata":{}},{"cell_type":"code","source":"def load_image(path, pad=32):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) / 255.0\n\n    h, w, _ = img.shape\n\n    pad_h = (pad - h % pad) % pad\n    pad_w = (pad - w % pad) % pad\n\n    img = np.pad(\n        img,\n        ((0, pad_h), (0, pad_w), (0, 0)),\n        mode=\"constant\",\n        constant_values=0\n    )\n\n    img = torch.from_numpy(img).permute(2, 0, 1)  # [3, H, W]\n    img = img.unsqueeze(0)                         # [1, 3, H, W]\n    return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:35.310796Z","iopub.execute_input":"2026-01-14T19:32:35.311362Z","iopub.status.idle":"2026-01-14T19:32:35.318334Z","shell.execute_reply.started":"2026-01-14T19:32:35.311328Z","shell.execute_reply":"2026-01-14T19:32:35.317344Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Inference + Submission","metadata":{}},{"cell_type":"code","source":"test_files = sorted(os.listdir(TEST_IMG_DIR))\n\ndef file_to_id(fname):\n    return os.path.splitext(fname)[0]   # removes .png\n\ntest_ids = [file_to_id(f) for f in test_files]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:35.319498Z","iopub.execute_input":"2026-01-14T19:32:35.320165Z","iopub.status.idle":"2026-01-14T19:32:35.341316Z","shell.execute_reply.started":"2026-01-14T19:32:35.320121Z","shell.execute_reply":"2026-01-14T19:32:35.340162Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# def load_image(path):\n#     img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n#     img = img.astype(np.float32) / 255.0\n#     img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)\n#     return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:35.342882Z","iopub.execute_input":"2026-01-14T19:32:35.343272Z","iopub.status.idle":"2026-01-14T19:32:35.347864Z","shell.execute_reply.started":"2026-01-14T19:32:35.343235Z","shell.execute_reply":"2026-01-14T19:32:35.346812Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# def load_image(path):\n#     img = cv2.imread(path)                 # BGR, 3-channel\n#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n#     img = img.astype(np.float32) / 255.0\n\n#     img = torch.from_numpy(img).permute(2, 0, 1)  # [3, H, W]\n#     img = img.unsqueeze(0)                         # [1, 3, H, W]\n#     return img","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:35.349120Z","iopub.execute_input":"2026-01-14T19:32:35.349503Z","iopub.status.idle":"2026-01-14T19:32:35.365371Z","shell.execute_reply.started":"2026-01-14T19:32:35.349433Z","shell.execute_reply":"2026-01-14T19:32:35.364254Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"rows = []\n\nmodel.eval()\nwith torch.no_grad():\n    for fname, id_str in zip(test_files, test_ids):\n        img_path = os.path.join(TEST_IMG_DIR, fname)\n\n        x = load_image(img_path).to(device)   # [1, 1, H, W]\n        y = model(x)                          # [1, C, H, W] or similar\n        print(y.shape)\n\n        # ---- DAY 1 REDUCTION (DO NOT CHANGE YET) ----\n        value = y.mean().item()               # scalar float\n\n        rows.append({\n            \"id\": id_str,\n            \"value\": float(value)\n        })\n\ndf = pd.DataFrame(rows, columns=[\"id\", \"value\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:32:35.366600Z","iopub.execute_input":"2026-01-14T19:32:35.366946Z","iopub.status.idle":"2026-01-14T19:38:19.508135Z","shell.execute_reply.started":"2026-01-14T19:32:35.366918Z","shell.execute_reply":"2026-01-14T19:38:19.506969Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 4, 864, 1104])\ntorch.Size([1, 4, 864, 1104])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:38:19.509285Z","iopub.execute_input":"2026-01-14T19:38:19.509611Z","iopub.status.idle":"2026-01-14T19:38:19.538289Z","shell.execute_reply.started":"2026-01-14T19:38:19.509584Z","shell.execute_reply":"2026-01-14T19:38:19.537287Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"           id     value\n0  1053922973 -0.594959\n1  2352854581 -0.594928","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1053922973</td>\n      <td>-0.594959</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2352854581</td>\n      <td>-0.594928</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"assert list(df.columns) == [\"id\", \"value\"]\nassert df.isnull().sum().sum() == 0\nassert len(df) == len(test_ids)\n\nprint(df.head())\nprint(df.dtypes)\nprint(\"Rows:\", len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:38:19.539504Z","iopub.execute_input":"2026-01-14T19:38:19.539926Z","iopub.status.idle":"2026-01-14T19:38:19.549220Z","shell.execute_reply.started":"2026-01-14T19:38:19.539885Z","shell.execute_reply":"2026-01-14T19:38:19.548202Z"}},"outputs":[{"name":"stdout","text":"           id     value\n0  1053922973 -0.594959\n1  2352854581 -0.594928\nid        object\nvalue    float64\ndtype: object\nRows: 2\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# @torch.no_grad()\n# def predict_mask(img_path):\n#     x = load_image(img_path).to(device)\n#     y = model(x)                  # [1, 4, H, W] logits\n\n#     probs = torch.sigmoid(y)      # independent channels\n\n#     ECG_CLASS = 1   # <-- try 1 first (see note below)\n#     ecg = probs[:, ECG_CLASS]     # [1, H, W]\n\n#     # mask = (ecg > 0.5).float()    # Day-1 threshold\n#     # keep only top X% most confident pixels\n#     thr = torch.quantile(ecg, 0.995)   # start with 99.5 percentile\n#     mask = (ecg >= thr).float()\n\n#     print(\n#     ecg.min().item(),\n#     ecg.mean().item(),\n#     ecg.max().item(),\n#     mask.mean().item()\n# )\n    \n\n\n#     return mask.squeeze().cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:38:19.550275Z","iopub.execute_input":"2026-01-14T19:38:19.550576Z","iopub.status.idle":"2026-01-14T19:38:19.567006Z","shell.execute_reply.started":"2026-01-14T19:38:19.550542Z","shell.execute_reply":"2026-01-14T19:38:19.565821Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# IMAGE_IDS = sorted(os.listdir(TEST_IMG_DIR))\n\n# records = []\n\n# for name in tqdm(IMAGE_IDS):\n#     img_path = os.path.join(TEST_IMG_DIR, name)\n\n#     mask = predict_mask(img_path)\n\n#     # simple Day-1 threshold\n#     binary = (mask > 0.5).astype(np.uint8)\n\n#     # flatten for submission (example format)\n#     rle = binary.flatten().tolist()\n\n#     records.append({\n#         \"image_id\": name,\n#         \"prediction\": \" \".join(map(str, rle))\n#     })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T19:38:19.568388Z","iopub.execute_input":"2026-01-14T19:38:19.568758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df = pd.DataFrame(records)\n# df.to_csv(OUT_SUB, index=False)\n# print(\"submission.csv saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df.iloc[0,1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# img_path = os.path.join(TEST_IMG_DIR, r'1053922973.png')\n\n# with torch.no_grad():\n#     x = load_image(img_path).to(device)\n#     y = model(x)\n#     # y = torch.sigmoid(y)\n\n# for c in range(4):\n#     print(c, y[0, c].mean().item(), y[0, c].max().item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}